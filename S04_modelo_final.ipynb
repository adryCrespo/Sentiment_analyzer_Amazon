{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c408de-5585-4dd9-8b5c-2a0e1303e7ef",
   "metadata": {},
   "source": [
    "MODELO FINAL \n",
    " --------------------\n",
    "El modelo ha sido probado con datos no vistos antes:    \n",
    "F1_TRAIN_SCORE: 0.7  \n",
    "F1_TEST_SCORE: 0.7  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1617e4e2-b1c1-4f33-b77f-fbee94b59b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utilities_dataset as uta\n",
    "import re\n",
    "#Automcompletar r√°pido\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "raiz= \"C:/Proyectos/X01_sentiment_analyzer\"\n",
    "\n",
    "path_stopwords = raiz+\"/01_Documentos/stopwords.txt\"\n",
    "stopwords = [c.strip() for c in open(path_stopwords)]\n",
    "\n",
    "datos = uta.load_dataset_train(raiz=raiz)\n",
    "datos_test = uta.load_dataset_val(raiz=raiz)\n",
    "\n",
    "Y = datos['sentiment'].copy()\n",
    "X = datos[['text']].fillna(\"-\").copy()\n",
    "\n",
    "\n",
    "Y_test = datos['sentiment'].copy()\n",
    "X_test = datos[['text']].fillna(\"-\").copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c48e1dd2-f990-492c-866b-2e4ce60f5b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentiment_Vectorizer:\n",
    "    def __init__(self, params_Tfidf=None ):\n",
    "        self.nombres_variables = None\n",
    "        self.params_Tfidf = params_Tfidf\n",
    "        self._get_parameters()\n",
    "        self.cv = None\n",
    "        self.num_variables = None\n",
    "        self.nombres_variables =None\n",
    "        \n",
    "    def _get_parameters(self):\n",
    "        if self.params_Tfidf is None:\n",
    "            self.params_Tfidf = {\"ngram_range\":(1, 1),\n",
    "                         \"min_df\":4,\n",
    "                         \"max_df\":0.5,             \n",
    "                         \"stop_words\": \"english\",\n",
    "                         \"smooth_idf\": True,\n",
    "                         \"use_idf\":True ,\n",
    "                         \"lowercase\":True,\n",
    "                         \"max_features\":300}\n",
    "            \n",
    "    def fit_vectorizer(self,data):\n",
    "        \"\"\"v1.0\"\"\"\n",
    "        X = data.copy()\n",
    "\n",
    "        cv=TfidfVectorizer(**self.params_Tfidf)\n",
    "        cv.fit(X['texto'])\n",
    "        \n",
    "        self.nombres_variables = cv.get_feature_names_out().tolist()\n",
    "        self.num_variables = len(self.nombres_variables)\n",
    "        self.cv = cv\n",
    "        return None\n",
    "\n",
    "    def transform_vectorizer(self, data):\n",
    "        dfs = self.cv.transform(data[\"texto\"])\n",
    "        return  pd.DataFrame.sparse.from_spmatrix(dfs,columns = self.nombres_variables  )\n",
    "        \n",
    "\n",
    "\n",
    "    def fit_transform_vectorizer(self,data):\n",
    "        self.fit_vectorizer(data)\n",
    "        return self.transform_vectorizer( data)\n",
    "\n",
    "class Sentiment_Evaluator:\n",
    "    def __init__(self, model, params_model=None):\n",
    "        \n",
    "        self.model = model\n",
    "        self.params_model = params_model\n",
    "        if params_model is None:\n",
    "            self.model = self.model()\n",
    "        else:\n",
    "            self.model = self.model(**params_model)\n",
    "        \n",
    "    def fit(self,train_data,Y, scoring=\"f1_macro\" , cv=10):\n",
    "        self.results = cross_val_score(self.model,train_data,Y,cv=cv,scoring=scoring)\n",
    "        self.scoring = scoring \n",
    "\n",
    "    \n",
    "    def print_cv_results(self):\n",
    "        print(\"RESULTADOS\")\n",
    "        print(\"---------------------\")\n",
    "        \n",
    "        media = np.mean(self.results)\n",
    "        std = np.std(self.results)\n",
    "        \n",
    "        print(f\"Metrica {self.scoring} \")\n",
    "        print(f\"media: {media:.3f}\")\n",
    "        print(f\"std:{std:.3f}\")\n",
    "        print(f\"min:{np.min(self.results):.3f}\")\n",
    "        print(f\"max:{np.max(self.results):.3f}\")\n",
    "\n",
    "        \n",
    "\n",
    "    def mostrar_tabla(self,train_data,Y,test_data=None):\n",
    "        self.model.fit(train_data,Y)\n",
    "        \n",
    "        from sklearn.metrics import ConfusionMatrixDisplay\n",
    "        \n",
    "        if test_data is None:\n",
    "            XX = train_data\n",
    "        else:\n",
    "            XX = test_data\n",
    "            \n",
    "        ConfusionMatrixDisplay.from_estimator( se_.model, XX, Y, xticks_rotation=\"vertical\" )\n",
    "        print(classification_report(Y, se_.model.predict(XX)))\n",
    "\n",
    "class New_features_pruebas:\n",
    "    def __init__(self, data, lista_fcs):\n",
    "        self.lista_fcs = lista_fcs\n",
    "        self.data = data\n",
    "        self.num_datos = len(data)\n",
    "        self.lista_output = []\n",
    "        \n",
    "    def _anadir_fc(self,nombre, fc):\n",
    "        \n",
    "        self._check_nombre(nombre)\n",
    "        self.lista_fcs.append( (nombre,fc))\n",
    "\n",
    "    def _check_nombre(self,nombre):\n",
    "        nombre_es_str = isinstance(nombre, str)\n",
    "        assert nombre_es_str, f\"nombre {nombre} tiene que ser string\"\n",
    "\n",
    "        \n",
    "    def fit(self,nombre, fc):\n",
    "        self._check_nombre(nombre)\n",
    "        \n",
    "        aux = self.data.apply(lambda x: fc(x))\n",
    "        aux.name = nombre\n",
    "        return aux\n",
    "            \n",
    "    def obtener_variables(self):\n",
    "        for nombre, fc in self.lista_fcs:\n",
    "            self.lista_output.append(self.fit(nombre,fc))\n",
    "        for elemento in self.lista_output:\n",
    "            assert elemento.size == self.num_datos, f\"numero de filas incorrecto, nombre: {elemento.name}\"\n",
    "\n",
    "        df_final = pd.concat(self.lista_output,axis=1)\n",
    "        return df_final\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "class Preprocesador_datos:\n",
    "    def __init__(self,data,variable):\n",
    "        \n",
    "        self.df_input = data[variable]\n",
    "        self.output =  None\n",
    "        self.nombre_final = 'texto'\n",
    "        self.lista_fcs = [self.feature_weburl]\n",
    "        self.stopwords = stopwords\n",
    "        \n",
    "    def feature_weburl(self,texto:str):\n",
    "        s = texto.lower().strip()\n",
    "        ss = re.sub(r\"(http|https)\\:\\/\\/\\w*\\.\\w*(\\.\\w*|\\/\\w*|~\\w*)*(|\\/[~\\w\\d]*|\\.w*)\", \"weburl\", s)\n",
    "        return ss\n",
    "        \n",
    "        \n",
    "    def preproceso_texto(self,texto:str):\n",
    "            s = texto.lower().strip()\n",
    "            porter = PorterStemmer()\n",
    "            tokens = nltk.tokenize.word_tokenize(s)\n",
    "            tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "            tokens = [porter.stem(word) for word in tokens]\n",
    "            return ' '.join(tokens)\n",
    "\n",
    "    def _add_fc(self,fcs):\n",
    "        self.lista_fcs.append(fcs)\n",
    "\n",
    "    def fit(self):\n",
    "        X = self.df_input.copy()\n",
    "        self._add_fc(self.preproceso_texto)\n",
    "        for fc in  self.lista_fcs:\n",
    "            X = self.apply_fc(X,fc)\n",
    "        self.output = X\n",
    "        \n",
    "    def apply_fc(self, data, fc):\n",
    "        return data.apply(lambda x: fc(x))\n",
    "        \n",
    "    @property\n",
    "    def datos_procesado(self):\n",
    "        aux =  pd.DataFrame(self.output)\n",
    "        aux.columns = [self.nombre_final]\n",
    "        return aux\n",
    "\n",
    "class tag_class:\n",
    "    def __init__(self, variables):\n",
    "        self.variables = variables \n",
    "        \n",
    "        \n",
    "    def get_tag(self, text):\n",
    "        return ' '.join([tag for word,  tag in nltk.pos_tag(nltk.word_tokenize(text))])\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        return len(re.findall(pattern, tweet))\n",
    "\n",
    "    def tag_proccess_var(self,var,data):\n",
    "        \"\"\" X : series\"\"\"\n",
    "        X.apply(lambda x : self.count_regex( var, data) )\n",
    "\n",
    "    def _normalizar_filas(self,X):\n",
    "        suma = X.sum()\n",
    "        return X/suma\n",
    "\n",
    "    def tag_proccess_df(self,X):\n",
    "        X_tag = X.apply(  lambda x : self.get_tag(x))\n",
    "        \n",
    "        self.output_list = []\n",
    "        for var in self.variables:\n",
    "            output = X_tag.apply(lambda x: self.count_regex(var,x))\n",
    "            output.name = var\n",
    "            self.output_list.append(output)\n",
    "        aux = pd.concat(self.output_list,axis=1)\n",
    "        return aux.apply(lambda x: self._normalizar_filas(x),axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81d383a-23f8-4176-b01f-a6039cd20f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cambio_contracciones(tweet):\n",
    "    \n",
    "    re_repl = {\n",
    "     r\"\\br\\b\": \"are\",\n",
    "     r\"\\bu\\b\": \"you\",\n",
    "     r\"\\bhaha\\b\": \"ha\",\n",
    "     r\"\\bhahaha\\b\": \"ha\",\n",
    "     r\"\\bdon`t\\b\": \"do not\",\n",
    "     r\"\\bdoesn`t\\b\": \"does not\",\n",
    "     r\"\\bdidn`t\\b\": \"did not\",\n",
    "     r\"\\bhasn`t\\b\": \"has not\",\n",
    "     r\"\\bhaven`t\\b\": \"have not\",\n",
    "     r\"\\bhadn`t\\b\": \"had not\",\n",
    "     r\"\\bwon`t\\b\": \"wont\",\n",
    "     r\"\\bwouldn`t\\b\": \"would not\",\n",
    "     r\"\\bcan`t\\b\": \"cant\",\n",
    "     r\"\\bcan` t\\b\": \"cant\",\n",
    "     r\"\\bcannot\\b\": \"cant\",\n",
    "     r\"\\bI`m\\b\": \"Im\",\n",
    "     r\"\\bI` m\\b\": \"Im\",\n",
    "     r\"\\bI am\\b\": \"Im\", \n",
    "     r\"\\bUGHHH*\\b\": \"ugh\", \n",
    "     r\"\\b[Uu]gh*\\b\": \"ugh\",\n",
    "     r\"\\bI` ve\\b\": \"I have\",  \n",
    "     r\"\\b[Ii]` s\\b\": \"it is \",\n",
    "     r\"\\b[Ii]`s\\b\": \"it is \",\n",
    "     r\"\\bborin\\b\": \"boring\",  \n",
    "     r\"\\bple+a+se\\b\": \"please\",  \n",
    "     r\"in`\": \"ing\",  \n",
    "     r\"\\bso+\\b\": \"so\", \n",
    "     r\"\\bso+w+y+\\b\": \"sorry\", \n",
    "     r\"\\b\\#\\w*\\b\": \"hastag\",\n",
    "     r\"[Yy]outube\":\" socialnetwork \",\n",
    "     r\"myspace\":\" socialnetwork \",\n",
    "     r\"twitpic\":\" socialnetwork \",\n",
    "     r\"latalkradio\":\" socialnetwork \",\n",
    "     r\"[Tt]witter\":\" socialnetwork \",\n",
    "     r\"[Tt]witch\":\" socialnetwork \",\n",
    "     r\"\\b: -\\\\/\\b\":\"emosad\",\n",
    "      \"#\\\\w*\":\"topico\",\n",
    "     r\"\\bstar wars\\b\":\" starwar \", \n",
    "     r\"\\bstar war\\b\":\" starwar \", \n",
    "     r\"\\bstar trek\\b\":\" startrek \",  \n",
    "     r\"\\bstar treck\\b\":\" startrek \",\n",
    "     # r\"\\bhappy mother day\\b\":\" happymotherday \",  \n",
    "     r\"\\bwan na\\b\": \" wanna \",\n",
    "     r\"\\bgot ta\\b\":\" gotta \",   \n",
    "     r\"\\b` ve\\b\": \" have \"}\n",
    "    for r, repl in re_repl.items():\n",
    "     tweet = re.sub(r, repl, tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def f_replace(text):\n",
    "    \n",
    "    text = text.replace(\"****\",\"fuck\")\n",
    "    return text\n",
    "    \n",
    "\n",
    "def conteo_ptos_suspensivos(ss):\n",
    "    return len(re.findall(\"...\",ss.lower() ))\n",
    "def conteo_exclamaciones(ss):\n",
    "    return len(re.findall(\"!\",ss.lower() ))\n",
    "\n",
    "def conteo_poor(ss):\n",
    "    return len(re.findall(\"poo+r\",ss.lower() ))\n",
    "\n",
    "def conteo_palabras(ss):\n",
    "    return len(ss.split())\n",
    "\n",
    "\n",
    "\n",
    "def conteo_mentions(ss):\n",
    "    return len(re.findall(r'@\\w+',ss.lower() ))\n",
    "\n",
    "def conteo_hashtags(ss):\n",
    "    return len(re.findall(r'#\\w+',ss.lower() ))\n",
    "\n",
    "def conteo_capital_words(ss):\n",
    "    return len(re.findall(r'\\b[A-Z]{2,}\\b',ss.lower() ))\n",
    "\n",
    "def conteo_digits(ss):\n",
    "    return len(re.findall(r'\\d+',ss.lower() ))\n",
    "\n",
    "\n",
    "def count_regex( pattern, tweet):\n",
    "        return len(re.findall(pattern, tweet))\n",
    "def cambio_contracciones_good(text):\n",
    "    emo_repl = {\n",
    "         # positive emoticons\n",
    "         \" &lt;3 \": \" emotigood \",\n",
    "         \" :d \": \" emotigood \", # :D in lower case\n",
    "         \" :dd \": \" emotigood \", # :DD in lower case\n",
    "         \" 8\\\\) \": \" emotigood \",\n",
    "         \" :-\\\\) \": \" emotigood \",\n",
    "         \" :\\\\) \": \" emotigood \",\n",
    "         \" ;\\\\) \": \" emotigood \",\n",
    "         \" \\\\(-: \": \" emotigood \",\n",
    "         \" \\\\(: \": \" emotigood \",\n",
    "         \" x[dD] \": \" emotigood \",\n",
    "    }\n",
    "         \n",
    "    emo_repl_order =  [k for (k_len,k) in reversed(sorted([(len(k),k) for k in emo_repl.keys()]))]\n",
    "    for k in emo_repl_order:\n",
    "        # text = re.sub().replace(k, emo_repl[k])\n",
    "        text = re.sub(k,  emo_repl[k], text)\n",
    "    return text\n",
    "\n",
    "def cambio_contracciones_bad(text):\n",
    "    emo_repl = {\n",
    "      \n",
    "         # negative emoticons:\n",
    "         \" :/ \": \" emotibad \",\n",
    "         \" :&gt; \": \" emotibad \",\n",
    "         \" :'\\\\)\": \" emotibad \",\n",
    "         \" :-\\\\( \": \" emotibad \",\n",
    "         \" :\\\\( \": \" emotibad \",\n",
    "         \" [Tt]_[Tt] \":\" emotibad \",\n",
    "         \" :S \" : \" emotibad \",\n",
    "         \" :-S \": \" emotibad \",\n",
    "     }\n",
    "    emo_repl_order =  [k for (k_len,k) in reversed(sorted([(len(k),k) for k in emo_repl.keys()]))]\n",
    "    for k in emo_repl_order:\n",
    "        # text = text.replace(k, emo_repl[k])\n",
    "        text = re.sub(k,  emo_repl[k], text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5e69f21-c6b6-4ba7-836e-9e488b0a684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesado_datos(X):\n",
    "\n",
    "    variables_finales_1_gram = ['broke', 'wish', 'hit', 'wrong', 'bad',\n",
    " 'mum','didnt','emotibad','name','re','monday','realli','site','outsid',\n",
    " 'boo','then','my','doe','angri','movi''week','rest','coffe','heard','hi','babi','around','excit','tonight',\n",
    " 'total','other','them','sorri','break','famili','enjoy','off','wow','saturday','dude','car','would','rock',\n",
    " 'mom','book','peopl','listen','isn','tomorrow','your','yay','start','hurt','read','that','exam','real','final',\n",
    " 'believ','nice','hate','job','ill','suck','help','funni','best','boy','dream','weather','bore','mother',\n",
    " 'get', 'word', 'welcom', 'trek', 'awesom', 'her', 'night', 'tire', 'photo', 'thi', 'eat', 'mayb', 'cant', 'readi',\n",
    " 'fan', 'live', 'dont', 'sooo', 'forgot', 'well', 'im', 'ladi', 'pretti', 'lost', 'crappi', 'couldn', 'dog', 'weburl','hot', 'good',\n",
    " 'left','so','ago','found','head','person','play','think','be','call','hear','holiday','till','ya','own',\n",
    " 'fuck','raini','lame','wa','not','weird','dinner','poor','win','honest','expens','veri','hello','gone',\n",
    " 'aww','fair','or','sick','super','glad','goodnight','world','internet','plan','cool','you','follow','kinda','too',\n",
    " 'starwar','tho','hour','le','cri','wait','class','startrek','leav','news','new','music','great','30','sunday','cute','crazi','war','god','take',\n",
    " 'dreari','depress','emotigood','trip','lunch','facebook','stop','stupid','until','lol','stuff','look','better','away','onli','hair','wanna','eye',\n",
    " 'pic','morn','ah','stress','brother','life','seen','mean','dad','thank','shop','ok','friend','omg','guy','but','hope','anymor','gut','am',\n",
    " 'birthday','beauti','sad','man','ugh','downfal','happi','forward','yeah','phone','such','soon','love','awww',\n",
    " 'wonder','caus','wasn','drink','busi','hey','late','ha','sadli','month','comput','onlin''meet','lot','finish','star','hous',\n",
    " 'day','feel','sound','pictur','talk','mommi','song','run','food', 'tri','hard','fun','bit','sweet','com','weekend',\n",
    " 'miss','parti','hug','hell','instead','oh','summer','rain','watch','tough','ur','ever','check','sleep','stuck','see','video','no','big','ye',\n",
    "  'free','amaz','post','cold','10','pleas','luck','offic','lmao','okay','disappoint','buy','come','friday','like','yesterday','headach','tell']\n",
    "\n",
    "    \n",
    "    params_model={\"max_iter\":2000}\n",
    "    \n",
    "    params_Tfidf = {\"ngram_range\":(1, 1),\n",
    "                             \"min_df\":4,\n",
    "                             \"max_df\":0.5,   \n",
    "                             \"smooth_idf\": True,\n",
    "                             \"use_idf\":True ,\n",
    "                             \"lowercase\":True,\n",
    "                             # \"max_features\":300 #,\n",
    "                            \"vocabulary\":variables_finales_1_gram\n",
    "                   }\n",
    "    \n",
    "    lista_funciones = [(\"conteo_exclamaciones\",conteo_exclamaciones),\n",
    "                       (\"conteo_ptos_suspensivos\",conteo_ptos_suspensivos) ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    preprocesador = Preprocesador_datos(X,\"text\")\n",
    "    preprocesador._add_fc(cambio_contracciones)\n",
    "    preprocesador._add_fc(cambio_contracciones_bad)\n",
    "    preprocesador._add_fc(cambio_contracciones_good)\n",
    "    preprocesador._add_fc(f_replace)\n",
    "    \n",
    "    \n",
    "    preprocesador.fit()\n",
    "    df_texto = preprocesador.datos_procesado\n",
    "    \n",
    "    \n",
    "    sv_ = Sentiment_Vectorizer(params_Tfidf )\n",
    "    x___ = sv_.fit_transform_vectorizer(preprocesador.datos_procesado)\n",
    "    \n",
    "    voc_2gram = [\"getting hungry\",\"too big\",\"looking forward\",\"cant wait\"  ]\n",
    "    params_Tfidf_2 = {\"ngram_range\":(2, 2), \n",
    "                             \"max_df\":0.5,   \n",
    "                             \"smooth_idf\": True,\n",
    "                             \"use_idf\":True ,\n",
    "                             \"lowercase\":True,\n",
    "                             \"max_features\":20,\n",
    "                      \"vocabulary\": voc_2gram\n",
    "                   }\n",
    "    sv_2 = Sentiment_Vectorizer(params_Tfidf_2 )\n",
    "    x___2 = sv_2.fit_transform_vectorizer(preprocesador.datos_procesado)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    tags = ['CC','CD',\n",
    "     'DT','EX','FW','IN','JJ',\n",
    "     'JJR','JJS','LS','MD',\n",
    "     'NN','NNP','NNPS','NNS','PDT',\n",
    "     'POS','PRP','RB','RBR','RBS',\n",
    "     'RP','SYM','TO','UH','VB','VBD',\n",
    "     'VBG','VBN','VBP','VBZ','WDT','WP','WRB']\n",
    "    \n",
    "    tag_instance = tag_class(tags)\n",
    "    X_tag = tag_instance.tag_proccess_df(X[\"text\"])\n",
    "    \n",
    "    \n",
    "    \n",
    "    nfp = New_features_pruebas(X[\"text\"], lista_funciones)\n",
    "    df_new_features = nfp.obtener_variables()\n",
    "    df_to_train = pd.concat([x___,x___2,X_tag.reset_index()[tags], df_new_features.reset_index()],axis=1).set_index(\"textID\")\n",
    "    columnas_finales = df_to_train.columns.tolist()\n",
    "    \n",
    "    return df_to_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5eff58-e9dd-4fae-b4c1-5aba15b8c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESADO DE TEXTO\n",
    "X_proc = procesado_datos(X)\n",
    "Y_test = datos['sentiment'].copy()\n",
    "X_test = datos[['text']].fillna(\"-\").copy()\n",
    "X_test_proc = procesado_datos(X_test)\n",
    "\n",
    "# ENTRENAMIENTO\n",
    "params_model={\"max_iter\":3000,'C':1,'class_weight': 'balanced'}\n",
    "log = LogisticRegression(**params_model)\n",
    "log.fit(X_proc,Y)\n",
    "\n",
    "# PREDECIR \n",
    "Y_train_predict = log.predict(X_proc)\n",
    "Y_test_predict = log.predict(X_test_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dded235f-5fee-426c-86c4-c0bd601b33b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_train_result = f1_score(Y, Y_train_predict, average='macro')\n",
    "f1_test_result = f1_score(Y_test, Y_test_predict, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc8b8de3-91c4-4ad5-a4b3-e13ef3d3a20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_train: 0.702, f1_test: 0.702\n"
     ]
    }
   ],
   "source": [
    "print(f\"f1_train: {f1_train_result:.3f}, f1_test: {f1_test_result:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b939da-8b44-4ab7-a8df-611be0b8225d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
